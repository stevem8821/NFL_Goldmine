{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the template to compile entire weeks' worth of URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls = []\n",
    "url_base = 'https://www.pro-football-reference.com/years/'\n",
    "url_tail = '.htm'\n",
    "years = []\n",
    "weeks = []\n",
    "\n",
    "for i in range(1,18,1):\n",
    "    string = 'week_'\n",
    "    number = str(i)\n",
    "    url = url_base + '2018' + '/' +  string + number + url_tail\n",
    "    urls.append(url)\n",
    "    years.append(2018)\n",
    "    weeks.append(i)\n",
    "\n",
    "for i in range(1,18,1):\n",
    "    string = 'week_'\n",
    "    number = str(i)\n",
    "    url = url_base + '2019' + '/' +  string + number + url_tail\n",
    "    urls.append(url)\n",
    "    years.append(2019)\n",
    "    weeks.append(i)\n",
    "    \n",
    "for i in range(1,18,1):\n",
    "    string = 'week_'\n",
    "    number = str(i)\n",
    "    url = url_base + '2020' + '/' +  string + number + url_tail\n",
    "    urls.append(url)\n",
    "    years.append(2020)\n",
    "    weeks.append(i)\n",
    "    \n",
    "for i in range(1,19,1):\n",
    "    string = 'week_'\n",
    "    number = str(i)\n",
    "    url = url_base + '2021' + '/' +  string + number + url_tail\n",
    "    urls.append(url)\n",
    "    years.append(2021)\n",
    "    weeks.append(i)\n",
    "    \n",
    "df_urls = pd.DataFrame()\n",
    "df_urls['urls'] = urls\n",
    "df_urls['years'] = years\n",
    "df_urls['weeks'] = weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetHrefs(url):\n",
    "    hrefs = []\n",
    "    driver = webdriver.Chrome(executable_path='chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    page = driver.execute_script('return document.body.innerHTML')\n",
    "    soup = BeautifulSoup(''.join(page), 'html.parser')\n",
    "    rows = soup.find_all('td', {'class':'right gamelink'})\n",
    "    for row in rows:\n",
    "        row2 = row.find('a')\n",
    "        hrefs.append(row2.get('href'))\n",
    "    return hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetURLs(urls,years,weeks):\n",
    "    lst = []\n",
    "    year = []\n",
    "    week = []\n",
    "    for i in range(0,len(urls)):\n",
    "        refs = GetHrefs(urls[i])\n",
    "        for item in refs:\n",
    "            lst.append('https://www.pro-football-reference.com' + item)\n",
    "            year.append(years[i])\n",
    "            week.append(weeks[i])\n",
    "\n",
    "    return lst, year, week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "urls = df_urls['urls']\n",
    "years = df_urls['years']\n",
    "weeks = df_urls['weeks']\n",
    "url_list, year_list, week_list = GetURLs(urls,years,weeks)\n",
    "\n",
    "df_final_urls = pd.DataFrame()\n",
    "df_final_urls['URLs'] = url_list\n",
    "df_final_urls['Year'] = year_list\n",
    "df_final_urls['Week'] = week_list\n",
    "df_final_urls.to_csv('URL_List.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
